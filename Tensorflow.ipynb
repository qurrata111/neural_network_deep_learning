{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction to TensorFlow</h2>\n",
    "<p>TensorFlow relies on a highly efficient C++ backend to do its computation. The connection to this backend is called a <b>Session</b>. The common usage for TensorFlow programs is to first create a <b>graph</b> and then launch it in a session.</p>\n",
    "\n",
    "<p>Tensorflow represent computations as graphs. Nodes in the graph are called <b>ops</b> (short for operations). An op takes tensors, performs some computation, and produces tensors. A <b>Tensor</b> is a typed multi-dimensional array. For example, you can represent a mini-batch of images as a 4-D array of floating point numbers with dimensions [batch, height, width, channels].</p>\n",
    "\n",
    "<p><b>Why use graph?</b> Instead of running a single expensive operation independently from Python, TF allows us to define a graph of interacting operations that run entirely outside Python to avoid the overhead from switching back to Python every operation. The role of the Python code is therefore to build this external computation graph, and to dictate which parts of the computation graph should be run. This approach is similar to that used in Theano or Torch</p>\n",
    "\n",
    "<p>A TensorFlow graph is a description of computations. To compute anything, a graph must be launched in a Session. A Session places the graph ops onto <b>Devices</b>, such as CPUs or GPUs, and provides methods to execute them. These methods return tensors produced by ops as numpy ndarray objects.</p>\n",
    "<br>\n",
    "<h3>Tensorboard</h3>\n",
    "<p><b>Initialize Tensorboard</b>: tensorboard --logdir=path/to/logs<b> Or specify another port 8008</b>: tensorboard --logdir=/tmp/mnist_logs_try --port=8008</p>\n",
    "<p><b>Compare train & test of your model:</b> output event summaries to /train and /test folders under path/to/logs</p>\n",
    "<p>To get an updated graph representation you should stop tensorboard and jupyter, delete your tensorflow logdir, restart jupyter, run the script, and then restart tensorboard.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['', '/Applications/Spyder-Py2.app/Contents/Resources', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python27.zip', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/plat-darwin', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/plat-mac', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/plat-mac/lib-scriptpackages', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/lib-tk', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/lib-old', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/lib-dynload', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/site-packages.zip', '/Applications/Spyder-Py2.app/Contents/Resources/lib/python2.7/site-packages', '/Library/Python/2.7/site-packages/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example1 - print out string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Scope_2/ScalarSummary/TensorSummary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant(12345) \n",
    "sess = tf.Session() # launch graph\n",
    "print(sess.run(hello)) \n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example2 - matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "# use 'with' to launch the default graph and close it automatically after use, to release the CPU/GPU resource\n",
    "# 'run' causes the execution of threes ops in the graph: the two constants and matmul\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example3 - fit a line to some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  w: [ 0.29914504]  b: [ 0.2643007]\n",
      "step: 50  w: [ 0.10568427]  b: [ 0.29688805]\n",
      "step: 100  w: [ 0.10019204]  b: [ 0.29989487]\n",
      "step: 150  w: [ 0.10000648]  b: [ 0.29999647]\n",
      "step: 200  w: [ 0.10000023]  b: [ 0.29999989]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "    x_data = np.random.rand(100).astype(np.float32)\n",
    "    y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "with tf.name_scope('parameters'):\n",
    "    # Try to find values for W and b that compute y_data = W * x_data + b\n",
    "    # (We know that W should be 0.1 and b 0.3, but TensorFlow will\n",
    "    # figure that out for us.)\n",
    "    W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\"weights\")\n",
    "    b = tf.Variable(tf.zeros([1]), name=\"bias\")\n",
    "\n",
    "with tf.name_scope('estimated_y'):\n",
    "    y = W * x_data + b\n",
    "\n",
    "# Minimize the mean squared errors.\n",
    "with tf.name_scope('MES_loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "    # create a summary for loss\n",
    "    tf.scalar_summary(\"loss\", loss)\n",
    "    tf.histogram_summary(\"loss_hist\", loss)\n",
    "with tf.name_scope('train'):\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # train operation\n",
    "\n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Instead of executing every summary operation individually we can merge them all together into a single merged summary operation.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    # Tensorflow summaries are essentially logs. And in order to write logs we need a log writer a SummaryWriter\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/tensorboard_example3\", graph=tf.get_default_graph())\n",
    "\n",
    "    # Fit the line.\n",
    "    for step in range(201):\n",
    "        #sess.run(train)\n",
    "        # perform the operations we defined earlier on batch\n",
    "        _, summary = sess.run([train_op, summary_op])\n",
    "\n",
    "        # write log\n",
    "        writer.add_summary(summary, step)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print 'step:',step, ' w:', sess.run(W), ' b:',sess.run(b)\n",
    "# Learns best fit is W: [0.1], b: [0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example4 - Softmax regression model with a single linear layer on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# download and read data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist is a class which stores the training, validation, and testing sets as NumPy arrays\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use InteractiveSession to allow interactive operations in IPython\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Start building the computation graph by creating nodes for the input images and target output classes.\n",
    "\n",
    "Here x and y_ aren't specific values. Rather, they are each a placeholder -- a value that we'll input when we ask TensorFlow to run a computation.\n",
    "\n",
    "The input images x will consist of a 2d tensor of floating point numbers. Here we assign it a shape of [None, 784], \n",
    "where 784 is the dimensionality of a single flattened 28 by 28 pixel MNIST image, and None indicates that the first dimension, \n",
    "corresponding to the batch size, can be of any size. The target output classes y_ will also consist of a 2d tensor, where each \n",
    "row is a one-hot 10-dimensional vector indicating which digit class (zero through nine) the corresponding MNIST image belongs to.\n",
    "'''\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model parameters as Variables, will be used in TensorFlow's computation graph\n",
    "# W is a 784x10 matrix (because we have 784 input features and 10 outputs)\n",
    "# b is a 10-dimensional vector (because we have 10 classes)\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before Variables can be used within a session, they must be initialized using that session. \n",
    "# This can be done for all Variables at once\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the unnormalized logit\n",
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, tf.log computes the logarithm of each element of y. Next, we multiply each element of y_ with the corresponding \n",
    "# element of tf.log(y). Then tf.reduce_sum adds the elements in the second dimension of y, due to the reduction_indices=[1] \n",
    "# parameter. Finally, tf.reduce_mean computes the mean over all the examples in the batch.\n",
    "# This is numerically unstable in practice, use below\n",
    "\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# tf.nn.softmax_cross_entropy_with_logits internally applies the softmax on the model's unnormalized model prediction \n",
    "# and calculate cross-entropy loss and sums across all classes\n",
    "# tf.reduce_mean returns the average over these sums, this gives a single number\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use gradient descent with learning rate = 0.5, to decrease cross-entropy loss.\n",
    "# It adds new operations to computation graph to compute gradients, compute parameter update steps, and apply update \n",
    "# steps to the parameters.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The returned train_step, when run, will apply the gradient descent updates to the parameters. \n",
    "# Training the model can therefore be accomplished by repeatedly running train_step.\n",
    "\n",
    "# Do training for 1000 epochs, with mini-batch size = 100\n",
    "# Use feed_dict to replace the placeholder tensors x and y_ with the training examples. Note that you can replace \n",
    "# any tensor in your computation graph using feed_dict -- it's not restricted to just placeholders.\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100) # batch is a tuple (mini_train, mini_label)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]}) # feed_dict: maps data onto Tensor placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9245\n"
     ]
    }
   ],
   "source": [
    "# Figure out where we predicted the correct label. \n",
    "# tf.argmax gives the index of the highest entry in a tensor along some axis. \n",
    "# e.g. tf.argmax(y,1) is the predicted label for each input instance, tf.argmax(y_,1) is the true label\n",
    "# tf.equal checks if our prediction matches the truth and gives boolean results: [True, False, True, True]\n",
    "correct_predictions = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# Cast boolean to float numbers and then take the mean, the second parameter specifies the new data type \n",
    "# For example, [True, False, True, True] would become [1,0,1,1] which have mean=0.75.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "# Evaluate our accuracy on the test data\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})) # feed_dict: maps data onto Tensor placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example5 - Softmax on MNIST with Tensorboard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy at epoch 0: 0.4075\n",
      "Accuracy at epoch 100: 0.8948\n",
      "Accuracy at epoch 200: 0.9031\n",
      "Accuracy at epoch 300: 0.9074\n",
      "Accuracy at epoch 400: 0.9037\n",
      "Accuracy at epoch 500: 0.9125\n",
      "Accuracy at epoch 600: 0.9148\n",
      "Accuracy at epoch 700: 0.9181\n",
      "Accuracy at epoch 800: 0.9166\n",
      "Accuracy at epoch 900: 0.9154\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# config\n",
    "batch_size = 100\n",
    "learning_rate = 0.5\n",
    "training_epochs = 1000\n",
    "logs_path = \"/tmp/mnist_softmax\"\n",
    "\n",
    "# different summaries for a single variable\n",
    "def variable_summaries(var, name):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.scalar_summary('stddev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary('histogram/' + name, var)\n",
    "\n",
    "# load mnist data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# input images\n",
    "with tf.name_scope('input'):\n",
    "    # None -> batch size can be any size, 784 -> flattened mnist image\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\") \n",
    "    # target 10 output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n",
    "\n",
    "with tf.name_scope(\"parameters\"):\n",
    "    # model parameters will change during training so we use tf.Variable\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W = tf.Variable(tf.zeros([784, 10]))\n",
    "        variable_summaries(W, 'weights')\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "        variable_summaries(b, 'bias')\n",
    "\n",
    "# implement model\n",
    "with tf.name_scope(\"softmax\"):\n",
    "    # y is our prediction\n",
    "    y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# specify cost function\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    # this is our cost\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    tf.scalar_summary(\"cost\", cross_entropy)\n",
    "    \n",
    "# specify optimizer\n",
    "with tf.name_scope('train'):\n",
    "    # optimizer is an \"operation\" which we can execute in a session\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.scalar_summary(\"accuracy\", accuracy_op)\n",
    "\n",
    "# merge all summaries into a single \"operation\" which we can execute in a session \n",
    "summary_op = tf.merge_all_summaries()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a log folder and save the graph structure, do this before training\n",
    "    #writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "    train_writer = tf.train.SummaryWriter(logs_path + '/train',graph=tf.get_default_graph())\n",
    "    test_writer = tf.train.SummaryWriter(logs_path + '/test')\n",
    "    \n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # write training summaries at every epoch\n",
    "        summary, _ = sess.run([summary_op, train_op], feed_dict={x: batch_x, y_: batch_y})\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "            \n",
    "        if epoch % batch_size == 0:  \n",
    "            # write testing summaries at every batch_size epochs\n",
    "            summary, acc = sess.run([summary_op, accuracy_op], feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_writer.add_summary(summary, epoch)\n",
    "            print('Accuracy at epoch %s: %s' % (epoch, acc))\n",
    "    \n",
    "    print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example6 - ConvNet on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the next block is the same as the first three steps from softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download and read data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist is a class which stores the training, validation, and testing sets as NumPy arrays\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# use InteractiveSession to allow interactive operations in IPython\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "'''\n",
    "Start building the computation graph by creating nodes for the input images and target output classes.\n",
    "\n",
    "Here x and y_ aren't specific values. Rather, they are each a placeholder -- a value that we'll input when we ask TensorFlow to run a computation.\n",
    "\n",
    "The input images x will consist of a 2d tensor of floating point numbers. Here we assign it a shape of [None, 784], \n",
    "where 784 is the dimensionality of a single flattened 28 by 28 pixel MNIST image, and None indicates that the first dimension, \n",
    "corresponding to the batch size, can be of any size. The target output classes y_ will also consist of a 2d tensor, where each \n",
    "row is a one-hot 10-dimensional vector indicating which digit class (zero through nine) the corresponding MNIST image belongs to.\n",
    "'''\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize weights and baises\n",
    "def weight_variable(shape):\n",
    "    # tf.truncated_normal returns random values from a normalal distribution and made sure no value exceeds 2 std\n",
    "    # shape must be a 1-D interger array\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1) \n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    # Since we use ReLU, we will initialize them with a slightly positive initial bias to avoid \"dead neurons\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions for convolution and pooling\n",
    "\n",
    "# Convolutions uses a stride of one and are zero padded so that the output is a tensor with the same size as the input. \n",
    "# x is input: must be of shape [batch, in_height, in_width, in_channels]\n",
    "# w is filter/kernel: be of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "# strides: A list of ints. 1-D of length 4\n",
    "# padding:  A string from: \"SAME\", \"VALID\". The type of padding algorithm to use\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Max pooling over 2x2 blocks on the input\n",
    "# x: A 4-D Tensor with shape [batch, height, width, channels] and type tf.float32\n",
    "# ksize: A list of ints that has length >= 4. The size of the window for each dimension of the input tensor z. In this case\n",
    "# it is 2*2 over height & width of each input instance\n",
    "# strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor. In this\n",
    "# case it is calculating one pixel out of every 2*2 pixels\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First convolutional layer, which is the result of convolution, followed by max pooling\n",
    "# [5, 5, 1, 32] indicates there are 32 filters with patch size 5*5 and input channel 1\n",
    "# therefore the output channel will be 32\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# have a bias term for each filter\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# To apply the layer, we first reshape x to a 4d tensor\n",
    "# The first dimension is -1 means the batch size is to be computed based on image width, height and channel, \n",
    "# The second and third dimensions correspond to image size 28 * 28\n",
    "# The final dimension corresponds to the number of color channels.\n",
    "# so that the total size remains constant\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # h means hidden\n",
    "h_pool1 = max_pool_2x2(h_conv1) # now activation map is 14 * 14 for each slice, and the whole tensor is batch_size*14*14*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Second convolutional layer\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64]) # 64 filters with patch size 5*5 and input channel 32\n",
    "b_conv2 = bias_variable([64]) # have a bias term for each filter\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) # now activation map is 7 * 7 for each slice, and the whole tensor is batch_size*7*7*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Densely/Fully connected layer\n",
    "\n",
    "# 7*7*64 is the dimension for the activation maps from the second convolutional layer\n",
    "# add a fully-connected layer with 1024 neurons to allow processing on the entire image\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024]) # one biase for each neuron on FC layer\n",
    "\n",
    "# Reshape the tensor from the pooling layer into a batch of vectors, each row vector is an activation map\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "# Multiply by a weight matrix, add a bias, and apply a ReLU\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply dropout before the readout/output layer. \n",
    "# keep_proba: a placeholder for the probability that a neuron's output is kept during dropout. \n",
    "# This allows us to turn dropout on during training, and turn it off during testing. \n",
    "# tf.nn.dropout op automatically handles scaling so dropout just works without any additional scaling.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Readout/Output layer\n",
    "\n",
    "# weights that connect FC layer with output layer\n",
    "# 10 output neurons since there are 10 classes\n",
    "W_output = weight_variable([1024, 10])\n",
    "b_output = bias_variable([10])\n",
    "\n",
    "# unnoralized logit\n",
    "y_conv = tf.matmul(h_fc1_drop, W_output) + b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training accuracy 0.12\n",
      "test accuracy 0.8374\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_)) # cross-entropy loss\n",
    "# used Adam optimizer instead of vanilla gradient descent to update parameters, learning rate = 0.0001\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) \n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) # boolean prediction results\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # prediction accuracy\n",
    "\n",
    "# Before Variables can be used within a session, they must be initialized using that session. \n",
    "# This can be done for all Variables at once\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# training with this many epochs\n",
    "for i in range(100): \n",
    "    # mini-batch size=50\n",
    "    # batch is a tuple (mini_train, mini_label)\n",
    "    batch = mnist.train.next_batch(50) \n",
    "    # print out accuracy on trainingset every 100 epochs\n",
    "    if i%100 == 0: \n",
    "        # note keep_prob for dropout is 1, which means no neurons is dropped out when evaluating the training accuracy\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})  # feed_dict: maps data onto Tensor placeholders\n",
    "        print(\"epoch %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    # note keep_prob for dropout is 0.5, which means half of neurons will be dropped out during training\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# print testing results\n",
    "# note keep_prob for dropout is 1, which means no neurons is dropped out during testing\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})) # feed_dict: maps data onto Tensor placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example7 - ConvNet on MNIST with Tensorboard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Test accuracy at epoch 0: 0.1155\n",
      "Test accuracy at epoch 50: 0.7281\n",
      "Test accuracy at epoch 100: 0.8236\n",
      "Test accuracy at epoch 150: 0.8727\n",
      "Test accuracy at epoch 200: 0.9059\n",
      "Test accuracy at epoch 250: 0.9075\n",
      "Test accuracy at epoch 300: 0.9239\n",
      "Test accuracy at epoch 350: 0.9288\n",
      "Test accuracy at epoch 400: 0.9344\n",
      "Test accuracy at epoch 450: 0.9425\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.name_scope(\"parameters\"):\\n    with tf.name_scope(\"weights\"):\\n        W = tf.Variable(tf.zeros([784, 10]))\\n        variable_summaries(W, \\'weights\\')\\n    with tf.name_scope(\"biases\"):\\n        b = tf.Variable(tf.zeros([10]))\\n        variable_summaries(b, \\'bias\\')\\n\\n# implement model\\nwith tf.name_scope(\"softmax\"):\\n    # y is our prediction\\n    y = tf.nn.softmax(tf.matmul(x,W) + b)\\n\\n# specify cost function\\nwith tf.name_scope(\\'cross_entropy\\'):\\n    # this is our cost\\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\\n    tf.scalar_summary(\"cost\", cross_entropy)\\n    \\n# specify optimizer\\nwith tf.name_scope(\\'train\\'):\\n    # optimizer is an \"operation\" which we can execute in a session\\n    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\\n\\nwith tf.name_scope(\\'Accuracy\\'):\\n    with tf.name_scope(\\'correct_prediction\\'):\\n        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\\n    with tf.name_scope(\\'accuracy\\'):\\n        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\\n        tf.scalar_summary(\"accuracy\", accuracy_op)\\n\\n# merge all summaries into a single \"operation\" which we can execute in a session \\nsummary_op = tf.merge_all_summaries()\\n\\nwith tf.Session() as sess:\\n    # create a log folder and save the graph structure, do this before training\\n    #writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\\n    train_writer = tf.train.SummaryWriter(logs_path + \\'/train\\',graph=tf.get_default_graph())\\n    test_writer = tf.train.SummaryWriter(logs_path + \\'/test\\')\\n    \\n    # variables need to be initialized before we can use them\\n    sess.run(tf.initialize_all_variables())\\n\\n    # perform training cycles\\n    for epoch in range(training_epochs):\\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\\n\\n        # write log\\n        summary, _ = sess.run([summary_op, train_op], feed_dict={x: batch_x, y_: batch_y})\\n        train_writer.add_summary(summary, epoch)\\n            \\n        if epoch % batch_size == 0:  # Record summaries and test-set accuracy\\n            summary, acc = sess.run([summary_op, accuracy_op], feed_dict={x: mnist.test.images, y_: mnist.test.labels})\\n            test_writer.add_summary(summary, epoch)\\n            print(\\'Accuracy at epoch %s: %s\\' % (epoch, acc))\\n    \\n    print \"done\"\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# config\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "training_epochs = 500\n",
    "logs_path = \"/tmp/mnist_convnet\"\n",
    "\n",
    "# different summaries for a single variable\n",
    "def variable_summaries(var, name):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.scalar_summary('stddev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary('histogram/' + name, var)\n",
    "\n",
    "# Initialize weights and baises\n",
    "def weight_variable(shape):\n",
    "    # tf.truncated_normal returns random values from a normalal distribution and made sure no value exceeds 2 std\n",
    "    # shape must be a 1-D interger array\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1) \n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    # Since we use ReLU, we will initialize them with a slightly positive initial bias to avoid \"dead neurons\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolutions uses a stride of one and are zero padded so that the output is a tensor with the same size as the input. \n",
    "# x is input: must be of shape [batch, in_height, in_width, in_channels]\n",
    "# w is filter/kernel: be of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "# strides: A list of ints. 1-D of length 4\n",
    "# padding:  A string from: \"SAME\", \"VALID\". The type of padding algorithm to use\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Max pooling over 2x2 blocks on the input\n",
    "# x: A 4-D Tensor with shape [batch, height, width, channels] and type tf.float32\n",
    "# ksize: A list of ints that has length >= 4. The size of the window for each dimension of the input tensor z. In this case\n",
    "# it is 2*2 over height & width of each input instance\n",
    "# strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor. In this\n",
    "# case it is calculating one pixel out of every 2*2 pixels\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# load mnist data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# input images\n",
    "with tf.name_scope('input'):\n",
    "    # None -> batch size can be any size, 784 -> flattened mnist image\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\") \n",
    "    # To apply the conv layer, we first reshape x to a 4d tensor\n",
    "    # The first dimension is -1 means the batch size is to be computed based on image width, height and channel, \n",
    "    # The second and third dimensions correspond to image size 28 * 28\n",
    "    # The final dimension corresponds to the number of color channels.\n",
    "    # so that the total size remains constant\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    \n",
    "    # target 10 output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n",
    "\n",
    "# First convolutional layer, which is the result of convolution, followed by max pooling\n",
    "with tf.name_scope('Conv1'):\n",
    "    with tf.name_scope('conv1_parameters'):\n",
    "        with tf.name_scope(\"conv1_weights\"):\n",
    "            # [5, 5, 1, 32] indicates there are 32 filters with size 5*5 and input channel 1\n",
    "            # therefore the output channel will be 32\n",
    "            W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "            variable_summaries(W_conv1, 'W_conv1')\n",
    "        with tf.name_scope(\"conv1_biases\"):\n",
    "            # have a bias term for each filter\n",
    "            b_conv1 = bias_variable([32])\n",
    "            variable_summaries(b_conv1, 'b_conv1')\n",
    "\n",
    "    with tf.name_scope('Conv1_activated_conv'):\n",
    "        # convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # h means hidden\n",
    "    with tf.name_scope('Conv1_max_pool'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # now activation map is 14 * 14 for each slice, and the whole tensor is batch_size*14*14*32\n",
    "\n",
    "\n",
    "# Second convolutional layer\n",
    "with tf.name_scope('Conv2'):\n",
    "    with tf.name_scope('conv2_parameters'):\n",
    "        with tf.name_scope(\"conv2_weights\"):\n",
    "            # [5, 5, 32, 64] indicates there are 64 filters with size 5*5 and input channel 32\n",
    "            # therefore the output channel will be 64\n",
    "            W_conv2 = weight_variable([5, 5, 32, 64]) \n",
    "            variable_summaries(W_conv2, 'W_conv2')\n",
    "        with tf.name_scope(\"conv2_biases\"):\n",
    "            b_conv2 = bias_variable([64]) # have a bias term for each filter\n",
    "            variable_summaries(b_conv2, 'b_conv2')\n",
    "    \n",
    "    with tf.name_scope('Conv2_activated_conv'):\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    with tf.name_scope('Conv2_max_pool'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2) # now activation map is 7 * 7 for each slice, and the whole tensor is batch_size*7*7*64        \n",
    "\n",
    "# Densely/Fully connected layer\n",
    "with tf.name_scope('FC_layer'):\n",
    "    with tf.name_scope('FC1_parameters'):\n",
    "        with tf.name_scope(\"FC1_weights\"):\n",
    "            # 7*7*64 is the dimension for the activation maps from the second convolutional layer\n",
    "            # add a fully-connected layer with 1024 neurons to allow processing on the entire image\n",
    "            W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "            variable_summaries(W_fc1, 'W_FC')\n",
    "        with tf.name_scope(\"FC1_biases\"):\n",
    "            b_fc1 = bias_variable([1024]) # one biase for each neuron on FC layer\n",
    "            variable_summaries(b_fc1, 'b_FC')\n",
    "\n",
    "    with tf.name_scope('FC1_flatten'):\n",
    "        # Reshape the tensor from the pooling layer into a batch of vectors, each row vector is an activation map\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    with tf.name_scope('FC1_activated'):\n",
    "        # Multiply by a weight matrix, add a bias, and apply a ReLU\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# apply dropout before the readout/output layer. \n",
    "# keep_proba: a placeholder for the probability that a neuron's output is kept during dropout. \n",
    "# This allows us to turn dropout on during training, and turn it off during testing. \n",
    "# tf.nn.dropout op automatically handles scaling so dropout just works without any additional scaling.\n",
    "with tf.name_scope('Dropout_layer'):    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Readout/Output layer\n",
    "with tf.name_scope('Output_layer'):\n",
    "    with tf.name_scope('output_parameters'):\n",
    "        with tf.name_scope(\"output_weights\"):\n",
    "            # weights that connect FC layer with output layer\n",
    "            # 10 output neurons since there are 10 classes\n",
    "            W_output = weight_variable([1024, 10])\n",
    "            variable_summaries(W_output, 'W_output')\n",
    "        with tf.name_scope(\"output_biases\"):\n",
    "            b_output = bias_variable([10])\n",
    "            variable_summaries(b_output, 'b_output')\n",
    "\n",
    "    with tf.name_scope('softmax'):\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_output) + b_output\n",
    "    \n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_)) # cross-entropy loss\n",
    "    tf.scalar_summary(\"cost\", cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    # used Adam optimizer instead of vanilla gradient descent to update parameters, learning rate = 0.0001\n",
    "    train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) \n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) # boolean prediction results\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # prediction accuracy\n",
    "        tf.scalar_summary(\"accuracy\", accuracy_op)    \n",
    "\n",
    "# merge all summaries into a single \"operation\" which we can execute in a session \n",
    "summary_op = tf.merge_all_summaries()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a log folder and save the graph structure, do this before training\n",
    "    #writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "    train_writer = tf.train.SummaryWriter(logs_path + '/train',graph=tf.get_default_graph())\n",
    "    test_writer = tf.train.SummaryWriter(logs_path + '/test')\n",
    "    \n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # write training summaries at every epoch\n",
    "        summary, _ = sess.run([summary_op, train_op], feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "            \n",
    "        # write testing summaries at every batch_size epoch\n",
    "        if epoch % batch_size == 0:  \n",
    "            summary, acc = sess.run([summary_op, accuracy_op], feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, epoch)\n",
    "            print('Test accuracy at epoch %s: %s' % (epoch, acc))\n",
    "    \n",
    "    print \"done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
